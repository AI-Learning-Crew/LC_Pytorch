"""
ëŒ€ì¡°í•™ìŠµì„ ìœ„í•œ VoxCeleb ë°ì´í„°ì…‹

ì´ ë°ì´í„°ì…‹ì€ ì–¼êµ´-ìŒì„± ë§¤ì¹­ì„ ìœ„í•œ ëŒ€ì¡°í•™ìŠµì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
ê° identityë³„ë¡œ ì´ë¯¸ì§€ì™€ ìŒì„±ì„ ëª…í™•íˆ ë§¤ì¹­í•˜ê³ , ë°°ì¹˜ ë‚´ì—ì„œ positive/negative pairë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
"""

import os
import json
import pickle
import random
import sys
from pathlib import Path
from typing import List, Dict, Tuple, Optional

import torch
from torch.utils.data import Dataset
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import librosa

# ìœˆë„ìš° í™˜ê²½ì—ì„œ UTF-8 ì¸ì½”ë”© ì§€ì›ì„ ìœ„í•œ ì„¤ì •
if sys.platform == "win32":
    import locale
    # ìœˆë„ìš°ì—ì„œ UTF-8 ì¸ì½”ë”© ê°•ì œ ì„¤ì •
    try:
        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ìœ¼ë¡œ UTF-8 ëª¨ë“œ í™œì„±í™”
        os.environ['PYTHONIOENCODING'] = 'utf-8'
        os.environ['PYTHONLEGACYWINDOWSSTDIO'] = '1'
        # ë¡œì¼€ì¼ ì„¤ì •
        locale.setlocale(locale.LC_ALL, 'ko_KR.UTF-8')
    except:
        try:
            # ëŒ€ì•ˆ ë¡œì¼€ì¼ ì„¤ì •
            locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')
        except:
            # ë¡œì¼€ì¼ ì„¤ì • ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
            pass


def safe_file_open(file_path, mode='r', encoding='utf-8'):
    """
    ìœˆë„ìš° í™˜ê²½ì—ì„œ íŠ¹ìˆ˜ ë¬¸ìê°€ í¬í•¨ëœ íŒŒì¼ëª…ì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        file_path: íŒŒì¼ ê²½ë¡œ
        mode: íŒŒì¼ ì—´ê¸° ëª¨ë“œ
        encoding: ì¸ì½”ë”© ë°©ì‹ (ë°”ì´ë„ˆë¦¬ ëª¨ë“œì—ì„œëŠ” ë¬´ì‹œë¨)
    
    Returns:
        íŒŒì¼ ê°ì²´
    """
    if sys.platform == "win32":
        try:
            # ë°”ì´ë„ˆë¦¬ ëª¨ë“œì¸ ê²½ìš° ì¸ì½”ë”© ì¸ì ì œì™¸
            if 'b' in mode:
                return open(file_path, mode)
            else:
                return open(file_path, mode, encoding=encoding)
        except (UnicodeDecodeError, UnicodeEncodeError):
            # ëŒ€ì•ˆ: íŒŒì¼ëª…ì„ ë°”ì´íŠ¸ë¡œ ì²˜ë¦¬
            if isinstance(file_path, str):
                file_path_bytes = file_path.encode('utf-8')
                if 'b' in mode:
                    return open(file_path_bytes, mode)
                else:
                    return open(file_path_bytes, mode, encoding=encoding)
            else:
                raise
    else:
        # ë°”ì´ë„ˆë¦¬ ëª¨ë“œì¸ ê²½ìš° ì¸ì½”ë”© ì¸ì ì œì™¸
        if 'b' in mode:
            return open(file_path, mode)
        else:
            return open(file_path, mode, encoding=encoding)


def safe_listdir(dir_path):
    """
    ìœˆë„ìš° í™˜ê²½ì—ì„œ íŠ¹ìˆ˜ ë¬¸ìê°€ í¬í•¨ëœ ë””ë ‰í† ë¦¬ë¥¼ ì•ˆì „í•˜ê²Œ ìŠ¤ìº”í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        dir_path: ë””ë ‰í† ë¦¬ ê²½ë¡œ
    
    Returns:
        íŒŒì¼/ë””ë ‰í† ë¦¬ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
    """
    if sys.platform == "win32":
        try:
            # ë¨¼ì € ì¼ë°˜ì ì¸ ë°©ë²• ì‹œë„
            return os.listdir(dir_path)
        except (UnicodeDecodeError, UnicodeEncodeError, OSError):
            # ëŒ€ì•ˆ: pathlib ì‚¬ìš©
            try:
                from pathlib import Path
                path_obj = Path(dir_path)
                return [item.name for item in path_obj.iterdir()]
            except Exception:
                # ìµœí›„ì˜ ìˆ˜ë‹¨: ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                print(f"ê²½ê³ : ë””ë ‰í† ë¦¬ '{dir_path}' ìŠ¤ìº” ì‹¤íŒ¨ - íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬ ë¶ˆê°€")
                return []
    else:
        return os.listdir(dir_path)


class ContrastiveVoxCelebDataset(Dataset):
    """ëŒ€ì¡°í•™ìŠµì„ ìœ„í•œ VoxCeleb ë°ì´í„°ì…‹
    
    ê° identityë³„ë¡œ ì´ë¯¸ì§€ì™€ ìŒì„±ì„ ëª…í™•íˆ ë§¤ì¹­í•˜ì—¬ ëŒ€ì¡°í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, 
                 voxceleb_root: str,
                 dataset_type: str = 'vox1',
                 split_type: str = 'train',
                 image_transform: Optional[transforms.Compose] = None,
                 audio_transform: Optional[callable] = None,
                 min_samples_per_identity: int = 2,
                 max_samples_per_identity: Optional[int] = None,
                 fixed_identities: Optional[List[str]] = None,
                 random_seed: int = 42):
        """
        ContrastiveVoxCelebDataset ì´ˆê¸°í™”
        
        Args:
            voxceleb_root: VoxCeleb ë°ì´í„° ë£¨íŠ¸ ë””ë ‰í† ë¦¬
            dataset_type: 'vox1' ë˜ëŠ” 'vox2'
            split_type: 'train', 'val', 'test'
            image_transform: ì´ë¯¸ì§€ ë³€í™˜ê¸°
            audio_transform: ì˜¤ë””ì˜¤ ë³€í™˜ê¸°
            min_samples_per_identity: identityë‹¹ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
            max_samples_per_identity: identityë‹¹ ìµœëŒ€ ìƒ˜í”Œ ìˆ˜
            random_seed: ëœë¤ ì‹œë“œ
        """
        self.voxceleb_root = Path(voxceleb_root)
        self.dataset_type = dataset_type
        self.split_type = split_type
        self.image_transform = image_transform
        self.audio_transform = audio_transform
        self.min_samples_per_identity = min_samples_per_identity
        self.max_samples_per_identity = max_samples_per_identity
        self.fixed_identities = fixed_identities
        self.random_seed = random_seed
        
        # ëœë¤ ì‹œë“œ ì„¤ì •
        random.seed(random_seed)
        
        # ë°ì´í„° ê²½ë¡œ ì„¤ì •
        # voxceleb_rootëŠ” ì´ë¯¸ HQ-VoxCelebê¹Œì§€ì˜ ê²½ë¡œ
        self.faces_dir = self.voxceleb_root / dataset_type / 'masked_faces'
        # ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ì€ ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ vox1/vox2ì— ìˆìŒ
        mel_root = self.voxceleb_root.parent.parent / dataset_type
        self.mel_dir = mel_root / 'mel_spectrograms'
        
        # ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
        if not self.faces_dir.exists():
            raise FileNotFoundError(f"ì–¼êµ´ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.faces_dir}")
        if not self.mel_dir.exists():
            raise FileNotFoundError(f"ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.mel_dir}")
        
        # ê° identityë³„ë¡œ ë°ì´í„° ìŠ¤ìº” ë° ìƒ˜í”Œ ìƒì„±
        self.samples = self._create_samples()
        print(f"Contrastive VoxCeleb {dataset_type} {split_type}: {len(self.samples)}ê°œ ìƒ˜í”Œ")
        print(f"ì´ë¯¸ì§€ì™€ ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ì´ ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ìŒë§Œ í•™ìŠµ ë°ì´í„°ì— í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    def _create_samples(self) -> List[Dict]:
        """ê° identityë³„ë¡œ ì´ë¯¸ì§€ì™€ ì˜¤ë””ì˜¤ë¥¼ ìŠ¤ìº”í•˜ì—¬ ìƒ˜í”Œ ìƒì„±"""
        samples = []
        
        # ëª¨ë“  identity ë””ë ‰í† ë¦¬ ìŠ¤ìº” (ì¸ì½”ë”© ë¬¸ì œ í•´ê²°)
        identity_dirs = []
        try:
            # ìœˆë„ìš°ì—ì„œ íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬ ê°œì„ 
            for d in self.faces_dir.iterdir():
                if d.is_dir():
                    identity_dirs.append(d)
        except (UnicodeDecodeError, UnicodeEncodeError) as e:
            print(f"ê²½ê³ : ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì¤‘ ì¸ì½”ë”© ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ëŒ€ì•ˆ ë°©ë²•: ì•ˆì „í•œ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì‚¬ìš©
            try:
                dir_names = safe_listdir(str(self.faces_dir))
                for dir_name in dir_names:
                    dir_path = self.faces_dir / dir_name
                    if dir_path.is_dir():
                        identity_dirs.append(dir_path)
            except Exception as e2:
                print(f"ê²½ê³ : ëŒ€ì•ˆ ë°©ë²•ë„ ì‹¤íŒ¨: {e2}")
                return []
        
        # identityë³„ë¡œ ìƒ˜í”Œ ìˆ˜ë¥¼ í™•ì¸í•˜ê³  ë¶„í• 
        identity_samples = {}
        
        for identity_dir in identity_dirs:
            try:
                identity = identity_dir.name
                
                # ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ (ì¸ì½”ë”© ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
                image_files = []
                try:
                    for ext in ['*.jpg', '*.jpeg', '*.png']:
                        image_files.extend(identity_dir.glob(ext))
                    image_files = [str(f) for f in image_files]
                except (UnicodeDecodeError, UnicodeEncodeError) as e:
                    print(f"ê²½ê³ : {identity} ì´ë¯¸ì§€ íŒŒì¼ ìŠ¤ìº” ì¤‘ ì¸ì½”ë”© ì˜¤ë¥˜: {e}")
                    # ëŒ€ì•ˆ ë°©ë²• ì‚¬ìš©: ì•ˆì „í•œ ë””ë ‰í† ë¦¬ ìŠ¤ìº”
                    try:
                        file_names = safe_listdir(str(identity_dir))
                        image_files = []
                        for file_name in file_names:
                            if file_name.lower().endswith(('.jpg', '.jpeg', '.png')):
                                image_files.append(str(identity_dir / file_name))
                    except Exception as e2:
                        print(f"ê²½ê³ : {identity} ëŒ€ì•ˆ ì´ë¯¸ì§€ ìŠ¤ìº”ë„ ì‹¤íŒ¨: {e2}")
                        continue
                
                # ì˜¤ë””ì˜¤ íŒŒì¼ ëª©ë¡ (ê°™ì€ identityì˜ ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ ë””ë ‰í† ë¦¬ì—ì„œ)
                mel_dir_path = self.mel_dir / identity
                
                # ìœˆë„ìš°ì—ì„œ íŠ¹ìˆ˜ë¬¸ì í¬í•¨ ë””ë ‰í† ë¦¬ ì¡´ì¬ ì—¬ë¶€ ì•ˆì „í•˜ê²Œ í™•ì¸
                mel_dir_exists = False
                actual_mel_dir_name = None
                
                try:
                    mel_dir_exists = mel_dir_path.exists()
                    if mel_dir_exists:
                        actual_mel_dir_name = identity
                except (UnicodeDecodeError, UnicodeEncodeError, OSError):
                    # ëŒ€ì•ˆ: ì•ˆì „í•œ ë””ë ‰í† ë¦¬ ìŠ¤ìº”ìœ¼ë¡œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    try:
                        mel_parent_files = safe_listdir(str(self.mel_dir))
                        # ì •í™•í•œ ë§¤ì¹­ì„ ìœ„í•´ ì—¬ëŸ¬ ë°©ë²• ì‹œë„
                        if identity in mel_parent_files:
                            mel_dir_exists = True
                            actual_mel_dir_name = identity
                        else:
                            # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”ë¥¼ í†µí•œ ë§¤ì¹­ ì‹œë„
                            import unicodedata
                            normalized_identity = unicodedata.normalize('NFC', identity)
                            for dir_name in mel_parent_files:
                                normalized_dir_name = unicodedata.normalize('NFC', dir_name)
                                if normalized_identity == normalized_dir_name:
                                    mel_dir_exists = True
                                    actual_mel_dir_name = dir_name
                                    break
                            
                            # ì—¬ì „íˆ ë§¤ì¹­ë˜ì§€ ì•Šìœ¼ë©´ ìœ ì‚¬í•œ ì´ë¦„ ì°¾ê¸°
                            if not mel_dir_exists:
                                # ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  ë§¤ì¹­
                                identity_lower = identity.lower()
                                for dir_name in mel_parent_files:
                                    if identity_lower == dir_name.lower():
                                        mel_dir_exists = True
                                        actual_mel_dir_name = dir_name
                                        break
                                
                                # ì—¬ì „íˆ ë§¤ì¹­ë˜ì§€ ì•Šìœ¼ë©´ ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
                                if not mel_dir_exists:
                                    for dir_name in mel_parent_files:
                                        # ê³µë°±ì´ë‚˜ ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ ì œê±°í•˜ê³  ë¹„êµ
                                        clean_identity = identity.replace('_', '').replace(' ', '').lower()
                                        clean_dir_name = dir_name.replace('_', '').replace(' ', '').lower()
                                        if clean_identity == clean_dir_name:
                                            mel_dir_exists = True
                                            actual_mel_dir_name = dir_name
                                            break
                                
                                # ì—¬ì „íˆ ë§¤ì¹­ë˜ì§€ ì•Šìœ¼ë©´ ë” ìœ ì—°í•œ ë§¤ì¹­ ì‹œë„
                                if not mel_dir_exists:
                                    # ì´ë¦„ì˜ ì£¼ìš” ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ì—¬ ë¹„êµ
                                    identity_parts = identity.replace('_', ' ').split()
                                    for dir_name in mel_parent_files:
                                        dir_parts = dir_name.replace('_', ' ').split()
                                        # ì£¼ìš” ë¶€ë¶„ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸ (ìµœì†Œ 2ê°œ ë¶€ë¶„ ì¼ì¹˜)
                                        common_parts = set(identity_parts) & set(dir_parts)
                                        if len(common_parts) >= min(2, len(identity_parts), len(dir_parts)):
                                            mel_dir_exists = True
                                            actual_mel_dir_name = dir_name
                                            break
                                
                                # ì—¬ì „íˆ ë§¤ì¹­ë˜ì§€ ì•Šìœ¼ë©´ ë¬¸ìì—´ ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­
                                if not mel_dir_exists:
                                    def levenshtein_distance(s1, s2):
                                        """ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê³„ì‚°"""
                                        if len(s1) < len(s2):
                                            return levenshtein_distance(s2, s1)
                                        if len(s2) == 0:
                                            return len(s1)
                                        
                                        previous_row = list(range(len(s2) + 1))
                                        for i, c1 in enumerate(s1):
                                            current_row = [i + 1]
                                            for j, c2 in enumerate(s2):
                                                insertions = previous_row[j + 1] + 1
                                                deletions = current_row[j] + 1
                                                substitutions = previous_row[j] + (c1 != c2)
                                                current_row.append(min(insertions, deletions, substitutions))
                                            previous_row = current_row
                                        return previous_row[-1]
                                    
                                    identity_clean = identity.replace('_', '').replace(' ', '').lower()
                                    best_match = None
                                    best_score = float('inf')
                                    
                                    for dir_name in mel_parent_files:
                                        dir_clean = dir_name.replace('_', '').replace(' ', '').lower()
                                        distance = levenshtein_distance(identity_clean, dir_clean)
                                        # ìœ ì‚¬ë„ê°€ ë†’ê³  ê¸¸ì´ê°€ ë¹„ìŠ·í•œ ê²½ìš°
                                        if distance < best_score and distance <= max(len(identity_clean), len(dir_clean)) * 0.3:
                                            best_score = distance
                                            best_match = dir_name
                                    
                                    if best_match:
                                        mel_dir_exists = True
                                        actual_mel_dir_name = best_match
                    except Exception as e:
                        print(f"ê²½ê³ : ë””ë ‰í† ë¦¬ ë§¤ì¹­ ì¤‘ ì˜¤ë¥˜: {e}")
                        mel_dir_exists = False
                
                if not mel_dir_exists:
                    print(f"ê²½ê³ : {identity}ì˜ ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                
                # ì‹¤ì œ ë””ë ‰í† ë¦¬ëª…ìœ¼ë¡œ ê²½ë¡œ ì¬ì„¤ì •
                if actual_mel_dir_name and actual_mel_dir_name != identity:
                    mel_dir_path = self.mel_dir / actual_mel_dir_name
                    print(f"{identity} -> {actual_mel_dir_name} ë””ë ‰í† ë¦¬ëª… ë§¤ì¹­ ì„±ê³µ")
                    
                mel_files = []
                try:
                    for ext in ['*.pickle']:
                        mel_files.extend(mel_dir_path.glob(ext))
                    mel_files = [str(f) for f in mel_files]
                except (UnicodeDecodeError, UnicodeEncodeError) as e:
                    print(f"ê²½ê³ : {identity} ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ íŒŒì¼ ìŠ¤ìº” ì¤‘ ì¸ì½”ë”© ì˜¤ë¥˜: {e}")
                    # ëŒ€ì•ˆ ë°©ë²• ì‚¬ìš©: ì•ˆì „í•œ ë””ë ‰í† ë¦¬ ìŠ¤ìº”
                    try:
                        file_names = safe_listdir(str(mel_dir_path))
                        mel_files = []
                        for file_name in file_names:
                            if file_name.lower().endswith('.pickle'):
                                mel_files.append(str(mel_dir_path / file_name))
                    except Exception as e2:
                        print(f"ê²½ê³ : {identity} ëŒ€ì•ˆ ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ ìŠ¤ìº”ë„ ì‹¤íŒ¨: {e2}")
                        continue
            except Exception as e:
                print(f"ê²½ê³ : {identity_dir} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue
            
            # ì´ë¯¸ì§€ì™€ ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ì´ ëª¨ë‘ ì¡´ì¬í•˜ëŠ”ì§€ ì—„ê²©í•˜ê²Œ í™•ì¸
            if not image_files:
                print(f"ê²½ê³ : {identity}ì— ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
                
            if not mel_files:
                print(f"ê²½ê³ : {identity}ì— ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            
            # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ í™•ì¸ (ì–‘ìª½ ëª¨ë‘ ì¡´ì¬í•´ì•¼ í•¨)
            min_samples = min(len(image_files), len(mel_files))
            if min_samples < self.min_samples_per_identity:
                print(f"ê²½ê³ : {identity}ì˜ ìƒ˜í”Œ ìˆ˜({min_samples})ê°€ ìµœì†Œ ìš”êµ¬ì‚¬í•­({self.min_samples_per_identity})ë³´ë‹¤ ì ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
                
            print(f"{identity}: ì´ë¯¸ì§€ {len(image_files)}ê°œ, ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ {len(mel_files)}ê°œ - í•™ìŠµ ë°ì´í„°ì— í¬í•¨")
            
            # identityë‹¹ ìƒ˜í”Œ ìˆ˜ ì œí•œ
            if self.max_samples_per_identity:
                max_samples = min(len(image_files), len(mel_files), self.max_samples_per_identity)
                image_files = random.sample(image_files, max_samples)
                mel_files = random.sample(mel_files, max_samples)
            
            # ìƒ˜í”Œ ìƒì„±: ê° identity ë‚´ì—ì„œ ì´ë¯¸ì§€ì™€ ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ì„ ë§¤ì¹­
            identity_samples[identity] = []
            for i in range(min_samples):
                # ê°™ì€ identity ë‚´ì—ì„œ ëœë¤í•˜ê²Œ ì„ íƒ
                image_path = random.choice(image_files)
                mel_path = random.choice(mel_files)
                
                # ì‹¤ì œ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                if not os.path.exists(image_path):
                    print(f"ê²½ê³ : ì´ë¯¸ì§€ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {image_path}")
                    continue
                if not os.path.exists(mel_path):
                    print(f"ê²½ê³ : ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {mel_path}")
                    continue
                
                identity_samples[identity].append({
                    'identity': identity,
                    'image_path': image_path,
                    'mel_path': mel_path
                })
        
        # Identity ëª©ë¡ ì¤€ë¹„
        all_identities = list(identity_samples.keys())
        
        # ê³ ì •ëœ identity ëª©ë¡ì´ ì£¼ì–´ì§„ ê²½ìš° ìš°ì„  ì‚¬ìš©
        if self.fixed_identities is not None:
            # ì¡´ì¬í•˜ëŠ” identityë§Œ í•„í„°ë§
            selected_identities = [i for i in self.fixed_identities if i in identity_samples]
        else:
            # ë¬´ì‘ìœ„ ë¶„í•  ìˆ˜í–‰
            identities = all_identities[:]
            random.shuffle(identities)
            
            # ë¶„í•  ë¹„ìœ¨: train 80%, val 10%, test 10%
            n_identities = len(identities)
            train_end = int(0.8 * n_identities)
            val_end = int(0.9 * n_identities)
            
            train_identities = identities[:train_end]
            val_identities = identities[train_end:val_end]
            test_identities = identities[val_end:]
            
            # split_typeì— ë”°ë¼ í•´ë‹¹í•˜ëŠ” identityë“¤ì˜ ìƒ˜í”Œë§Œ ì„ íƒ
            if self.split_type == 'train':
                selected_identities = train_identities
            elif self.split_type == 'val':
                selected_identities = val_identities
            elif self.split_type == 'test':
                selected_identities = test_identities
            else:
                # ì „ì²´ ì‚¬ìš©
                selected_identities = identities
        
        # ì„ íƒëœ identityë“¤ì˜ ìƒ˜í”Œì„ ëª¨ë‘ ì¶”ê°€
        for identity in selected_identities:
            samples.extend(identity_samples[identity])
        
        print(f"ë°ì´í„° ë¶„í• : {self.split_type} - {len(selected_identities)}ê°œ identity, {len(samples)}ê°œ ìƒ˜í”Œ")
        print(f"ğŸ“Š ìœ íš¨í•œ identity ëª©ë¡: {selected_identities[:5]}{'...' if len(selected_identities) > 5 else ''}")
        
        return samples
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        """ë°ì´í„°ì…‹ì—ì„œ ì•„ì´í…œ ê°€ì ¸ì˜¤ê¸°"""
        sample = self.samples[idx]
        
        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ì¬í™•ì¸
        if not os.path.exists(sample['image_path']):
            print(f"ê²½ê³ : ì´ë¯¸ì§€ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {sample['image_path']}")
            # ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±
            image_tensor = torch.zeros(3, 224, 224)
        elif not os.path.exists(sample['mel_path']):
            print(f"ê²½ê³ : ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {sample['mel_path']}")
            # ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±
            image_tensor = torch.zeros(3, 224, 224)
        else:
            # ì´ë¯¸ì§€ ë¡œë“œ ë° ë³€í™˜ (ì¸ì½”ë”© ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
            try:
                image = Image.open(sample['image_path']).convert("RGB")
                if self.image_transform:
                    image_tensor = self.image_transform(image)
                else:
                    # ê¸°ë³¸ ë³€í™˜
                    image_tensor = transforms.ToTensor()(image)
            except Exception as e:
                print(f"ì´ë¯¸ì§€ ë¡œë“œ ì˜¤ë¥˜: {sample['image_path']} - {e}")
                # ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±
                image_tensor = torch.zeros(3, 224, 224)
        
        # ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ ë¡œë“œ (ì¸ì½”ë”© ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
        mel_path = sample['mel_path']
        
        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ì¬í™•ì¸
        if not os.path.exists(mel_path):
            print(f"ê²½ê³ : ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {mel_path}")
            # ê¸°ë³¸ ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ ìƒì„±
            mel_tensor = torch.zeros(1, 40, 100)
        else:
            try:
                # ì•ˆì „í•œ íŒŒì¼ ì—´ê¸° ì‚¬ìš©
                with safe_file_open(mel_path, 'rb') as f:
                    mel_data = pickle.load(f)
            
                # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° 'mel' í‚¤ë¡œ ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì¶”ì¶œ
                if isinstance(mel_data, dict):
                    if 'mel' in mel_data:
                        mel_tensor = torch.tensor(mel_data['mel'], dtype=torch.float32)
                    elif 'mel_spectrogram' in mel_data:
                        mel_tensor = torch.tensor(mel_data['mel_spectrogram'], dtype=torch.float32)
                    else:
                        # ë”•ì…”ë„ˆë¦¬ì˜ ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
                        first_key = list(mel_data.keys())[0]
                        mel_tensor = torch.tensor(mel_data[first_key], dtype=torch.float32)
                else:
                    # ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° ì§ì ‘ í…ì„œë¡œ ë³€í™˜
                    mel_tensor = torch.tensor(mel_data, dtype=torch.float32)
                
                # 2Dë¥¼ 3Dë¡œ ë³€í™˜ (ì±„ë„ ì°¨ì› ì¶”ê°€)
                if mel_tensor.dim() == 2:
                    mel_tensor = mel_tensor.unsqueeze(0)  # (1, freq, time)
                    
            except Exception as e:
                print(f"ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ ë¡œë“œ ì˜¤ë¥˜: {mel_path} - {e}")
                # ê¸°ë³¸ ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ ìƒì„±
                mel_tensor = torch.zeros(1, 40, 100)
        
        return {
            'image': image_tensor,
            'mel': mel_tensor,
            'identity': sample['identity']
        }


def collate_contrastive_fn(batch):
    """
    ëŒ€ì¡°í•™ìŠµì„ ìœ„í•œ collate í•¨ìˆ˜
    
    ë°°ì¹˜ ë‚´ì—ì„œ ê°™ì€ identityì˜ ì—¬ëŸ¬ ìƒ˜í”Œì„ í¬í•¨í•˜ì—¬ positive pairë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # Identityë³„ë¡œ ê·¸ë£¹í™”
    identity_groups = {}
    for item in batch:
        identity = item['identity']
        if identity not in identity_groups:
            identity_groups[identity] = []
        identity_groups[identity].append(item)
    
    # ê° identityì—ì„œ ì—¬ëŸ¬ ìƒ˜í”Œì„ ì„ íƒí•˜ì—¬ ê· í˜• ì¡íŒ ë°°ì¹˜ êµ¬ì„±
    balanced_batch = []
    identities = list(identity_groups.keys())
    
    if len(identities) == 0:
        return {
            'image': torch.stack([item['image'] for item in batch]),
            'mel': torch.stack([item['mel'] for item in batch]),
            'identity': [item['identity'] for item in batch]
        }
    
    # ì›ë˜ ë°°ì¹˜ êµ¬ì„± (ìì—°ìŠ¤ëŸ¬ìš´ positive pair ìƒì„±)
    # ê° identityì—ì„œ ìµœì†Œ 1ê°œì”© ìƒ˜í”Œì„ ì„ íƒí•˜ë˜, ê°€ëŠ¥í•˜ë©´ 2ê°œì”© ì„ íƒ
    samples_per_identity = max(1, len(batch) // len(identities))
    
    # ë¨¼ì € ê° identityì—ì„œ ê¸°ë³¸ ìƒ˜í”Œ ìˆ˜ë§Œí¼ ì„ íƒ
    for identity in identities:
        samples = identity_groups[identity]
        for _ in range(min(samples_per_identity, len(samples))):
            balanced_batch.append(random.choice(samples))
    
    # ë°°ì¹˜ í¬ê¸°ê°€ ë¶€ì¡±í•˜ë©´ ì¶”ê°€ ìƒ˜í”Œë¡œ ì±„ì›€
    while len(balanced_batch) < len(batch):
        identity = random.choice(identities)
        balanced_batch.append(random.choice(identity_groups[identity]))
    
    # ë°°ì¹˜ í¬ê¸°ê°€ ì´ˆê³¼í•˜ë©´ ìë¥´ê¸°
    balanced_batch = balanced_batch[:len(batch)]
    
    # ë°°ì¹˜ êµ¬ì„± í™•ì¸ (ë¡œê¹… ì œê±°)
    identity_counts = {}
    for item in balanced_batch:
        identity = item['identity']
        identity_counts[identity] = identity_counts.get(identity, 0) + 1
    
    positive_pairs = sum(1 for count in identity_counts.values() if count >= 2)
    # ë¡œê¹… ì œê±° (ë„ˆë¬´ ë§ì€ ì¶œë ¥ ë°©ì§€)
    
    # ì´ë¯¸ì§€ì™€ ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ ìŠ¤íƒ
    images = torch.stack([item['image'] for item in balanced_batch])
    
    # ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ íŒ¨ë”© ì²˜ë¦¬
    mels = [item['mel'] for item in balanced_batch]
    
    # ê° ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ì„ (freq, time) í˜•íƒœë¡œ ë³€í™˜
    mels_2d = []
    for mel in mels:
        if mel.dim() == 3:  # (1, freq, time)
            mel = mel.squeeze(0)  # (freq, time)
        mels_2d.append(mel)
    
    # 2D í…ì„œ íŒ¨ë”©ì„ ìœ„í•œ ìµœëŒ€ í¬ê¸° ì°¾ê¸°
    max_freq = max(mel.shape[0] for mel in mels_2d)
    max_time = max(mel.shape[1] for mel in mels_2d)
    
    # íŒ¨ë”©ëœ í…ì„œ ìƒì„±
    mels_padded = []
    for mel in mels_2d:
        # (freq, time) -> (max_freq, max_time)ë¡œ íŒ¨ë”©
        padded_mel = torch.zeros(max_freq, max_time)
        padded_mel[:mel.shape[0], :mel.shape[1]] = mel
        mels_padded.append(padded_mel)
    
    # ë°°ì¹˜ë¡œ ìŠ¤íƒí•˜ê³  (batch, 1, freq, time) í˜•íƒœë¡œ ë³€í™˜
    mels_padded = torch.stack(mels_padded).unsqueeze(1)
    
    identities = [item['identity'] for item in balanced_batch]
    
    return {
        'image': images,
        'mel': mels_padded,
        'identity': identities
    }


def create_contrastive_transforms(use_augmentation: bool = True, image_size: int = 224):
    """
    ëŒ€ì¡°í•™ìŠµìš© ì´ë¯¸ì§€ ë³€í™˜ê¸° ìƒì„±
    """
    transform_list = [
        transforms.Resize((image_size, image_size)),
    ]
    
    if use_augmentation:
        transform_list.extend([
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
            transforms.RandomRotation(degrees=10),
        ])
    
    transform_list.extend([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    return transforms.Compose(transform_list)


def create_contrastive_voxceleb_dataloaders(voxceleb_root: str,
                                           dataset_types: List[str] = ['vox1'],
                                           batch_size: int = 16,
                                           num_workers: int = 4,
                                           image_size: int = 224,
                                           min_samples_per_identity: int = 2,
                                           max_samples_per_identity: Optional[int] = None,
                                           random_seed: int = 42,
                                           split_save_dir: Optional[str] = None,
                                           split_load_dir: Optional[str] = None):
    """
    ëŒ€ì¡°í•™ìŠµìš© VoxCeleb ë°ì´í„°ë¡œë”ë“¤ì„ ìƒì„±
    
    Args:
        voxceleb_root: VoxCeleb ë°ì´í„° ë£¨íŠ¸ ë””ë ‰í† ë¦¬
        dataset_types: ì‚¬ìš©í•  ë°ì´í„°ì…‹ íƒ€ì… ë¦¬ìŠ¤íŠ¸
        batch_size: ë°°ì¹˜ í¬ê¸°
        num_workers: ë°ì´í„° ë¡œë”© ì›Œì»¤ ìˆ˜
        image_size: ì´ë¯¸ì§€ í¬ê¸°
        min_samples_per_identity: identityë‹¹ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
        max_samples_per_identity: identityë‹¹ ìµœëŒ€ ìƒ˜í”Œ ìˆ˜
        random_seed: ëœë¤ ì‹œë“œ
        
    Returns:
        train, val ë°ì´í„°ë¡œë” ë”•ì…”ë„ˆë¦¬
    """
    from torch.utils.data import DataLoader, ConcatDataset
    
    # ë³€í™˜ê¸° ìƒì„±
    train_transform = create_contrastive_transforms(use_augmentation=True, image_size=image_size)
    val_transform = create_contrastive_transforms(use_augmentation=False, image_size=image_size)
    
    # ê° ë°ì´í„°ì…‹ íƒ€ì…ë³„ë¡œ ë°ì´í„°ì…‹ ìƒì„±
    train_datasets = []
    val_datasets = []
    test_datasets = []
    
    for dataset_type in dataset_types:
        print(f"ëŒ€ì¡°í•™ìŠµ ë°ì´í„°ì…‹ {dataset_type} ë¡œë”© ì¤‘...")
        
        # ê³ ì • split ë¡œë“œ (ì¡´ì¬í•˜ë©´ ì‚¬ìš©) - ì¸ì½”ë”© ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        fixed_splits = None
        if split_load_dir is not None:
            split_path = Path(split_load_dir) / f"contrastive_split_{dataset_type}.json"
            if split_path.exists():
                try:
                    # ì•ˆì „í•œ íŒŒì¼ ì—´ê¸° ì‚¬ìš©
                    with safe_file_open(str(split_path), 'r', encoding='utf-8') as f:
                        fixed_splits = json.load(f)
                    print(f"ê³ ì • split ë¡œë“œ: {split_path}")
                except Exception as e:
                    print(f"ê²½ê³ : split íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {split_path} - {e}")
        
        # í›ˆë ¨ ë°ì´í„°ì…‹
        train_dataset = ContrastiveVoxCelebDataset(
            voxceleb_root=voxceleb_root,
            dataset_type=dataset_type,
            split_type='train',
            image_transform=train_transform,
            min_samples_per_identity=min_samples_per_identity,
            max_samples_per_identity=max_samples_per_identity,
            fixed_identities=(fixed_splits['train'] if fixed_splits and 'train' in fixed_splits else None),
            random_seed=random_seed
        )
        train_datasets.append(train_dataset)
        
        # ê²€ì¦ ë°ì´í„°ì…‹
        val_dataset = ContrastiveVoxCelebDataset(
            voxceleb_root=voxceleb_root,
            dataset_type=dataset_type,
            split_type='val',
            image_transform=val_transform,
            min_samples_per_identity=min_samples_per_identity,
            max_samples_per_identity=max_samples_per_identity,
            fixed_identities=(fixed_splits['val'] if fixed_splits and 'val' in fixed_splits else None),
            random_seed=random_seed
        )
        val_datasets.append(val_dataset)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹
        test_dataset = ContrastiveVoxCelebDataset(
            voxceleb_root=voxceleb_root,
            dataset_type=dataset_type,
            split_type='test',
            image_transform=val_transform,  # í…ŒìŠ¤íŠ¸ëŠ” augmentation ì—†ì´
            min_samples_per_identity=min_samples_per_identity,
            max_samples_per_identity=max_samples_per_identity,
            fixed_identities=(fixed_splits['test'] if fixed_splits and 'test' in fixed_splits else None),
            random_seed=random_seed
        )
        test_datasets.append(test_dataset)

        # ê³ ì • split ì €ì¥ (ìš”ì²­ëœ ê²½ìš°, ê·¸ë¦¬ê³  ì´ë²ˆì— ë¬´ì‘ìœ„ ë¶„í• ì„ ìƒì„±í•œ ê²½ìš°)
        if split_save_dir is not None and fixed_splits is None:
            # ë°©ê¸ˆ ìƒì„±í•œ ë¶„í• ì„ ì €ì¥í•˜ê¸° ìœ„í•´ datasetsì—ì„œ identity ëª©ë¡ ìˆ˜ì§‘
            def collect_identities(ds: ContrastiveVoxCelebDataset) -> List[str]:
                # samplesì—ì„œ identityë¥¼ ìˆ˜ì§‘í•˜ì—¬ unique ì •ë ¬
                return sorted(list({s['identity'] for s in ds.samples}))
            to_save = {
                'train': collect_identities(train_dataset),
                'val': collect_identities(val_dataset),
                'test': collect_identities(test_dataset)
            }
            Path(split_save_dir).mkdir(parents=True, exist_ok=True)
            out_path = Path(split_save_dir) / f"contrastive_split_{dataset_type}.json"
            try:
                # ì•ˆì „í•œ íŒŒì¼ ì—´ê¸° ì‚¬ìš©
                with safe_file_open(str(out_path), 'w', encoding='utf-8') as f:
                    json.dump(to_save, f, ensure_ascii=False, indent=2)
                print(f"ê³ ì • split ì €ì¥: {out_path}")
            except Exception as e:
                print(f"ê²½ê³ : split ì €ì¥ ì‹¤íŒ¨: {out_path} - {e}")
    
    # ì—¬ëŸ¬ ë°ì´í„°ì…‹ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
    if len(train_datasets) > 1:
        train_dataset = ConcatDataset(train_datasets)
        val_dataset = ConcatDataset(val_datasets)
        test_dataset = ConcatDataset(test_datasets)
        print(f"ì—¬ëŸ¬ ë°ì´í„°ì…‹ í†µí•©: {dataset_types}")
    else:
        train_dataset = train_datasets[0]
        val_dataset = val_datasets[0]
        test_dataset = test_datasets[0]
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=collate_contrastive_fn,
        num_workers=num_workers,
        pin_memory=True
    )
    
    val_dataloader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=collate_contrastive_fn,
        num_workers=num_workers,
        pin_memory=True
    )
    
    test_dataloader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=collate_contrastive_fn,
        num_workers=num_workers,
        pin_memory=True
    )
    
    return {
        'train': train_dataloader,
        'val': val_dataloader,
        'test': test_dataloader
    }
